{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importowanie bibliotek i wczytanie ścieżek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions: ['happy', 'sad', 'neutral', 'angry']\n"
     ]
    }
   ],
   "source": [
    "# Importowanie bibliotek\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Sequential, load_model  # Dodano `load_model`\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "\n",
    "# Ścieżki do folderów z danymi\n",
    "train_path = \"./images/train-kopia/\"\n",
    "val_path = \"./images/validation/\"\n",
    "\n",
    "# Rozmiar obrazów\n",
    "pic_size = 48\n",
    "\n",
    "# Lista emocji (kategorii)\n",
    "# Lista emocji (kategorii) - pominięcie pliku .DS_Store\n",
    "emotions = [folder for folder in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, folder))]\n",
    "print(\"Emotions:\", emotions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych treningowych i walidacyjnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie danych treningowych\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_path = os.path.join(train_path, emotion)\n",
    "    if not os.path.isdir(emotion_path):\n",
    "        continue\n",
    "\n",
    "    for img_name in os.listdir(emotion_path):\n",
    "        if img_name == '.DS_Store':\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(emotion_path, img_name)\n",
    "        img = load_img(img_path, target_size=(pic_size, pic_size), color_mode='grayscale')\n",
    "        img_array = img_to_array(img)\n",
    "        train_data.append(img_array)\n",
    "        train_labels.append(emotions.index(emotion))  # Przypisanie etykiety numerycznej\n",
    "\n",
    "# Przygotowanie danych walidacyjnych\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_path = os.path.join(val_path, emotion)\n",
    "    if not os.path.isdir(emotion_path):\n",
    "        continue\n",
    "\n",
    "    for img_name in os.listdir(emotion_path):\n",
    "        if img_name == '.DS_Store':\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(emotion_path, img_name)\n",
    "        img = load_img(img_path, target_size=(pic_size, pic_size), color_mode='grayscale')\n",
    "        img_array = img_to_array(img)\n",
    "        val_data.append(img_array)\n",
    "        val_labels.append(emotions.index(emotion))  # Przypisanie etykiety numerycznej\n",
    "\n",
    "# Normalizacja danych\n",
    "train_data = np.array(train_data, dtype=\"float32\") / 255.0\n",
    "train_labels = np.array(train_labels)\n",
    "val_data = np.array(val_data, dtype=\"float32\") / 255.0\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "# One-hot encoding etykiet\n",
    "train_labels = to_categorical(train_labels, len(emotions))\n",
    "val_labels = to_categorical(val_labels, len(emotions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie modelu i trenowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.3728 - loss: 1.3287 - val_accuracy: 0.5230 - val_loss: 1.1035\n",
      "Epoch 2/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.5495 - loss: 1.0501 - val_accuracy: 0.5739 - val_loss: 0.9971\n",
      "Epoch 3/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6043 - loss: 0.9417 - val_accuracy: 0.6089 - val_loss: 0.9443\n",
      "Epoch 4/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6296 - loss: 0.8804 - val_accuracy: 0.6200 - val_loss: 0.9077\n",
      "Epoch 5/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.6599 - loss: 0.8180 - val_accuracy: 0.6391 - val_loss: 0.8892\n",
      "Epoch 6/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.6769 - loss: 0.7793 - val_accuracy: 0.6321 - val_loss: 0.9006\n",
      "Epoch 7/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.6982 - loss: 0.7386 - val_accuracy: 0.6436 - val_loss: 0.8932\n",
      "Epoch 8/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.7158 - loss: 0.7017 - val_accuracy: 0.6467 - val_loss: 0.8765\n",
      "Epoch 9/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.7368 - loss: 0.6633 - val_accuracy: 0.6475 - val_loss: 0.8901\n",
      "Epoch 10/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.7614 - loss: 0.6038 - val_accuracy: 0.6479 - val_loss: 0.9106\n",
      "Epoch 11/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.7725 - loss: 0.5780 - val_accuracy: 0.6518 - val_loss: 0.9419\n",
      "Epoch 12/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8005 - loss: 0.5082 - val_accuracy: 0.6385 - val_loss: 0.9835\n",
      "Epoch 13/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8182 - loss: 0.4683 - val_accuracy: 0.6496 - val_loss: 1.0113\n",
      "Epoch 14/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.8389 - loss: 0.4231 - val_accuracy: 0.6514 - val_loss: 1.0923\n",
      "Epoch 15/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.8533 - loss: 0.3905 - val_accuracy: 0.6352 - val_loss: 1.1486\n",
      "Epoch 16/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8723 - loss: 0.3374 - val_accuracy: 0.6296 - val_loss: 1.2072\n",
      "Epoch 17/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.8949 - loss: 0.2911 - val_accuracy: 0.6235 - val_loss: 1.3360\n",
      "Epoch 18/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.9067 - loss: 0.2561 - val_accuracy: 0.6354 - val_loss: 1.3240\n",
      "Epoch 19/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.9126 - loss: 0.2303 - val_accuracy: 0.6304 - val_loss: 1.4504\n",
      "Epoch 20/20\n",
      "\u001b[1m659/659\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.9241 - loss: 0.2144 - val_accuracy: 0.6381 - val_loss: 1.6312\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7541 - loss: 1.1181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6311655044555664\n",
      "Validation Accuracy: 0.6381322741508484\n"
     ]
    }
   ],
   "source": [
    "# Tworzenie modelu CNN\n",
    "model = Sequential()\n",
    "\n",
    "# Warstwy konwolucyjne\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(pic_size, pic_size, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Spłaszczenie\n",
    "model.add(Flatten())\n",
    "\n",
    "# Warstwy gęste (fully connected)\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(len(emotions), activation='softmax'))  # Wyjście: liczba emocji\n",
    "\n",
    "# Kompilowanie modelu\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie modelu\n",
    "history = model.fit(train_data, train_labels, epochs=20, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "# Ocena modelu na danych walidacyjnych\n",
    "val_loss, val_accuracy = model.evaluate(val_data, val_labels)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Zapisanie modelu\n",
    "model.save(\"emotion_recognition_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie modelu i funkcja predict_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Wczytanie zapisanego modelu\n",
    "model = load_model(\"emotion_recognition_model.h5\")\n",
    "\n",
    "def predict_emotion(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Przekształcanie obrazu na skalę szarości\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')  # Ładowanie klasyfikatora twarzy\n",
    "    \n",
    "    # Zwiększenie wartości scaleFactor i minNeighbors, aby zmniejszyć fałszywe wykrycia\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=4, minSize=(30, 30))  # Wykrywanie twarzy\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]  # Wycinanie twarzy\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # Konwertowanie twarzy na skalę szarości\n",
    "        face = cv2.resize(face, (pic_size, pic_size))  # Zmiana rozmiaru twarzy\n",
    "        face = face.reshape(1, pic_size, pic_size, 1)  # Reshape twarzy do formatu (1, 48, 48, 1)\n",
    "        face = face / 255.0  # Normalizacja obrazu\n",
    "        \n",
    "        # Przewidywanie emocji\n",
    "        prediction = model.predict(face)\n",
    "        max_index = np.argmax(prediction[0])\n",
    "        emotion_predicted = emotions[max_index]\n",
    "        \n",
    "        # Wyświetlanie wyniku\n",
    "        cv2.putText(frame, emotion_predicted, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uruchamianie kamery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Uruchomienie kamery\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Rozpoczęcie liczenia czasu\u001b[39;00m\n\u001b[1;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "# Uruchomienie kamery\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Rozpoczęcie liczenia czasu\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Odczytanie obrazu z kamery\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Przewidywanie emocji na obrazie\n",
    "    frame = predict_emotion(frame)\n",
    "    \n",
    "    # Wyświetlanie obrazu z wykrytymi emocjami\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "    \n",
    "    # Sprawdzanie, czy upłynęło 30 sekund\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if elapsed_time > 60:\n",
    "        break  # Przerwij po 30 sekundach\n",
    "    \n",
    "    # Zatrzymanie kamery po naciśnięciu 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()  # Zwolnienie kamery\n",
    "cv2.destroyAllWindows()  # Zamykanie okien"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
